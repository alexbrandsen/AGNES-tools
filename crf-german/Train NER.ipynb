{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c1b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn_crfsuite\n",
      "  Cloning https://github.com/MeMartijn/updated-sklearn-crfsuite.git to c:\\users\\ewout\\appdata\\local\\temp\\pip-install-m8t2mo23\\sklearn-crfsuite_a01288a330d7441599b37e66c74f5c8d\n",
      "  Resolved https://github.com/MeMartijn/updated-sklearn-crfsuite.git to commit 675038761b4405f04691a83339d04903790e2b95\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sklearn_crfsuite) (4.62.3)\n",
      "Requirement already satisfied: six in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sklearn_crfsuite) (1.12.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sklearn_crfsuite) (0.9.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from sklearn_crfsuite) (0.9.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm>=2.0->sklearn_crfsuite) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/MeMartijn/updated-sklearn-crfsuite.git 'C:\\Users\\ewout\\AppData\\Local\\Temp\\pip-install-m8t2mo23\\sklearn-crfsuite_a01288a330d7441599b37e66c74f5c8d'\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: C:\\Users\\ewout\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit_learn in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit_learn) (1.21.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit_learn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit_learn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit_learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: C:\\Users\\ewout\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (2021.9.24)\n",
      "Requirement already satisfied: joblib in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: C:\\Users\\ewout\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.12.1)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from seaborn) (1.21.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.31.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (8.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas>=0.25->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\ewout\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: C:\\Users\\ewout\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#install required packages\n",
    "%pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
    "%pip install scikit_learn\n",
    "%pip install nltk\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn_crfsuite\n",
    "\n",
    "#from matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from itertools\n",
    "from itertools import chain\n",
    "\n",
    "#from sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function turns the file into a list. \n",
    "def file2list(fileLocation):\n",
    "    outputList = []\n",
    "    with open(fileLocation, 'r', encoding='utf8') as myfile:\n",
    "        sentences = myfile.read().split('\\n\\n')\n",
    "        for sentence in sentences:\n",
    "                sentenceList = []\n",
    "                words = sentence.split('\\n')\n",
    "                for word in words:\n",
    "                    wordsList = []\n",
    "                    attributes = word.split(' ')\n",
    "                    for attribute in attributes:\n",
    "                        wordsList.append(attribute)\n",
    "                    sentenceList.append(wordsList)\n",
    "                outputList.append(sentenceList)\n",
    "    \n",
    "    return outputList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12.1', 'NOUN', 'O'], [')', 'PUNCT', 'O'], ['.', 'PUNCT', 'O']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# train =  file2list('D:\\\\phd-data\\\\NER-annotation-data\\\\Dutch\\\\5-folds-with-pos-with-cut-sentences\\\\fold1.txt') \n",
    "# test =  file2list('D:\\\\phd-data\\\\NER-annotation-data\\\\Dutch\\\\5-folds-with-pos-with-cut-sentences\\\\fold2.txt')\n",
    "\n",
    "#load the datasets, with training documents as train, and test documents and test\n",
    "train =  file2list('data/train.bio') \n",
    "test =  file2list('data/test.bio')\n",
    "\n",
    "#remove empty line at the end of the file as this breaks the code\n",
    "test.pop() \n",
    "train.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Linearbandkeramik', 'NOUN', 'B-ART'],\n",
       " ['aus', 'ADP', 'O'],\n",
       " ['Meindling', 'PROPN', 'B-LOC'],\n",
       " [',', 'PUNCT', 'O'],\n",
       " ['Gem', 'PROPN', 'O'],\n",
       " ['.', 'PUNCT', 'O']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "# calculates the time to open the file\n",
    "        \n",
    "#train the NER on the list. there is one set of test and one of training. often 20:80 split\n",
    "train_sent = train\n",
    "test_sent = test   # tests the sent (input) of the given list as defined above\n",
    "train_sent[0] # displayes the first 10 rows in the bio. - each row hs the token (effectively word), followed by pos?, and the bio label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jüngere Eisenzeit', 'Mittel Neolithikum', 'Mittlere Bronzezeit', 'Mittelpaläolithikum', 'Ältere Kupferzeit', 'Mesolithikum', 'Hochmittelalter', 'Zeitgeschichte', 'Älteres Jungpaläolithikum', 'Spätpaläolithikum', 'Frühe Bronzezeit', 'Frühes Neolithikum', 'Mittlere Kupferzeit', 'Zeitgeschichte', 'Mittleres Jungpaläolithikum', 'Eisenzeit', 'Mittelpaläolithikum', 'Römische', 'Frühe Römische', 'Paläolithikum', 'Späte Bronzezeit', 'Späte Römische', 'Mesolithikum', 'Ältere Kupferzeit', 'Neuzeit', 'Frühe Bronzezeit', 'Völkerwanderungszeit', 'Mittel Neolithikum', 'Späte Römische', 'Paläolithikum', 'Späte Bronzezeit', 'Mittelalter', 'Mittelalter', 'Spätmittelalter', 'Frühes Neolithikum', 'Älteres Jungpaläolithikum', 'Neuzeit', 'Frühe Römische', 'Spätpaläolithikum', 'Jüngeres Jungpaläolithikum', 'Jüngeres Jungpaläolithikum', 'Bronzezeit', 'Kupferzeit', 'Hochmittelalter', 'Mittleres Jungpaläolithikum', 'Jüngere Kupferzeit', 'Frühes Mittelalter', 'Frühes Mittelalter', 'Mittlere Kupferzeit', 'Jüngere Kupferzeit', 'Neolithikum', 'Neolithikum', 'Mittlere Bronzezeit', 'Völkerwanderungszeit', 'Eisenzeit', 'Römische', 'Kupferzeit', 'Spätmittelalter', 'Ältere Eisenzeit', 'Bronzezeit', 'Alter Styl', 'Strenger Styl', 'Zweiten Periode', 'Vorgerückten Kunstsitte', 'Chalkolithikum', 'Geometrische Zeit', 'frühgeometrisch', 'mittelgeometrisch', 'spätgeometrisch', 'Archaische Epoche', 'früharchaisch', 'mittelarchaisch', 'spätarchaisch', 'Klassische Epoche', 'Strenger Stil', 'Hochklassik', 'Reicher Stil', 'Spätklassik', 'Hellenistische Epoche', 'frühhellenistisch', 'hochhellenistisch', 'späthellenistisch', 'Früheisenzeit', 'Archaische Zeit', 'Republikanische Zeit', 'frührepublikanisch', 'mittelrepublikanisch', 'spätrepublikanisch', 'Frühe Kaiserzeit', 'Mittlere Kaiserzeit', 'Späte Kaiserzeit', 'Spätantike', 'Frühe Ägäische Bronzezeit I-III', 'Mittlere Ägäische Bronzezeit', 'MM / MH I-II', 'MM / MH III', 'Späte Ägäische Bronzezeit', 'SM I A / SH I', 'SM I B / SH II A', 'SM II / SH II B']\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncontext = pd.read_csv(r\\'ontologies\\\\context.csv\\')\\ncontext[context.columns[0]]= context[context.columns[0]].str.lower().str.replace(\\'*\\',\\'\\').str.replace(\\'<\\',\\'\\').str.replace(\\'>\\',\\'\\') #replace all other symbols\\ncontext[context.columns[0]]= context[context.columns[0]].replace(to_replace =\\':.*\\',value=\\'\\',regex=True) # replace the words after a colon\\ncontext_list = context[context.columns[0]].values.tolist()\\nprint(context_list[0:100])\\nprint(\"\\n\"\"\\n\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time to load the ontologies \n",
    "\"\"\"\n",
    "#Material ontology\n",
    "materials = pd.read_csv(r'ontologies\\Materials.csv') #open the file\n",
    "materials[materials.columns[2]]= materials[materials.columns[2]].str.lower() #takes the second column (the one with the entities) and makes it all lower case\n",
    "materials_list = materials[materials.columns[2]].values.tolist() #turns the items into a list\n",
    "print(materials_list) # outputs the list of terms to see what sort of data it contains\n",
    "print(\"\\n\"\"\\n\")\n",
    "\n",
    "#Taxon ontology\n",
    "# taxon = pd.read_csv(r'ontologies\\Taxon.tsv', sep=\"\\t\", error_bad_lines=False) #open the file\n",
    "# taxon[taxon.columns[8]]= taxon[taxon.columns[8]].str.lower() #takes the second column (the one with the entities) and makes it all lower case\n",
    "# taxon_list = taxon[taxon.columns[8]].values.tolist() #turns the items into a list\n",
    "# print(taxon_list[0:100]) # outputs the list of terms to see what sort of data it contains\n",
    "# print(\"\\n\"\"\\n\")\n",
    "\n",
    "#Animal ontology\n",
    "animal = pd.read_csv(r'ontologies\\VernacularName.tsv', sep=\"\\t\") #open the file\n",
    "animal[animal.columns[2]]= animal[animal.columns[2]].str.lower() #takes the second column (the one with the entities) and makes it all lower case\n",
    "animal_list = animal[animal.columns[2]].values.tolist() #turns the items into a list\n",
    "print(animal_list[0:100]) # outputs the list of terms to see what sort of data it contains\n",
    "print(\"\\n\"\"\\n\")\n",
    "\"\"\"\n",
    "\n",
    "#Periods ontology - for notes see above\n",
    "#periods = pd.read_csv(r'ontologies\\Periods.csv')\n",
    "#periods[periods.columns[1]]= periods[periods.columns[1]].str.lower()\n",
    "#periods_list = periods[periods.columns[1]].values.tolist()\n",
    "periods_list = []\n",
    "with open(r\"ontologies\\periodo-dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        periods_list.append(i.strip('\\n'))\n",
    "print(periods_list[0:100])\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#Context ontology - for notes see material \n",
    "\"\"\"\n",
    "context = pd.read_csv(r'ontologies\\context.csv')\n",
    "context[context.columns[0]]= context[context.columns[0]].str.lower().str.replace('*','').str.replace('<','').str.replace('>','') #replace all other symbols\n",
    "context[context.columns[0]]= context[context.columns[0]].replace(to_replace =':.*',value='',regex=True) # replace the words after a colon\n",
    "context_list = context[context.columns[0]].values.tolist()\n",
    "print(context_list[0:100])\n",
    "print(\"\\n\"\"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn each word into a list of features\n",
    "def word2features(sent, i): \n",
    "    word = sent[i][0] #this is the token\n",
    "    postag = sent[i][1] #this is the Part of Speech tag\n",
    "    \n",
    "     #this tells if the token is in the ontology or not \n",
    "#    if word in materials_list: \n",
    "#        in_materials = True \n",
    "#    else:\n",
    "#        in_materials = False\n",
    "    \n",
    "#     if word in taxon_list: \n",
    "#         in_taxon = True \n",
    "#     else:\n",
    "#         in_taxon = False\n",
    "    \n",
    "#    if word in animal_list: \n",
    "#        in_animal = True \n",
    "#    else:\n",
    "#        in_animal = False\n",
    "  \n",
    "    if word in periods_list: \n",
    "        in_periods = True \n",
    "    else:\n",
    "        in_periods = False\n",
    "\n",
    "#    if word in context_list: \n",
    "#        in_context = True \n",
    "#    else:\n",
    "#        in_context = False\n",
    "    \n",
    "    #time to give each token some information     \n",
    "    features = { # these are all default. \n",
    "        'bias': 1.0, # bias is just 1. \n",
    "        'word.lower()': word.lower(), # tells if the token is lower case \n",
    "        'word[-3:]': word[-3:], # takes the last four letters - the suffix\n",
    "#        'Word.in_materials': in_materials, #is the token in the material ontology\n",
    "#         'Word.in_taxon': in_taxon, #is the token in the material ontology\n",
    "#         'Word.in_animal': in_animal, #is the token in the periods ontology \n",
    "#        'Word.in_periods': in_periods, #is the token in the periods ontology\n",
    "#        'Word.in_context': in_context, #is the token in the evidence ontology\n",
    "        'word.isupper()': word.isupper(), # tells if the whole token is uppercase \n",
    "        'word.istitle()': word.istitle(), # tells if the token is capital first letter\n",
    "        'postag': postag,  # what is its label - Part-Of-Speech Tagger\n",
    "        'postag[:2]': postag[:2],  #Takes the first three letters of the tag\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0: #if the token is not at the start of a sentence\n",
    "        word1 = sent[i-1][0] # works out details of the token before - this is to understand the context \n",
    "        postag1 = sent[i-1][1] #what is the postag of the word before\n",
    "        wordbefore = (sent[i-1][0]+ ' ' +sent[i][0]).lower #this is the token and the token before\n",
    "        #if this word and word before is in the ontology then \n",
    "#        if wordbefore in materials_list: \n",
    "#            wordbefore_in_materials = True \n",
    "#        else:\n",
    "#            wordbefore_in_materials = False\n",
    "\n",
    "#         if wordbefore in taxon_list: \n",
    "#             wordbefore_in_taxon = True \n",
    "#         else:\n",
    "#             wordbefore_in_taxon = False\n",
    "\n",
    "#        if wordbefore in animal_list: \n",
    "#            wordbefore_in_animal = True \n",
    "#            print(word)\n",
    "#        else:\n",
    "#            wordbefore_in_animal = False\n",
    "\n",
    "        if wordbefore in periods_list: \n",
    "            wordbefore_in_periods = True \n",
    "        else:\n",
    "            wordbefore_in_periods = False\n",
    "\n",
    "#        if wordbefore in context_list: \n",
    "#            wordbefore_in_context = True \n",
    "#        else:\n",
    "#            wordbefore_in_context = False\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(), # tells if the token is lower case\n",
    "            '-1:word.istitle()': word1.istitle(), # tells if the token is capital first letter\n",
    "            '-1:word.isdigit()': word1.isdigit(), # tells if the toekn is only numbers\n",
    "            '-1:word.isupper()': word1.isupper(),# tells if the whole token is uppercase\n",
    "#            'wordbefore_in_materials': wordbefore_in_materials,\n",
    "#             'wordbefore_in_taxon': wordbefore_in_taxon,\n",
    "#             'wordbefore_in_animal': wordbefore_in_animal,\n",
    "#            'wordbefore_in_periods': wordbefore_in_periods,\n",
    "#            'wordbefore_in_context': wordbefore_in_context,\n",
    "            '-1:postag': postag1, # what was its POS tag\n",
    "            '-1:postag[:2]': postag1[:2], #what is the first three POS tag of the word before\n",
    "        })\n",
    "    \n",
    "        if i > 1: #if the token is not at the start of a sentence\n",
    "                word3 = sent[i-1][0] # works out details of the token before - this is to understand the context \n",
    "                postag3 = sent[i-1][1] #what is the postag of the word before\n",
    "                two_words_before = (sent[i-1][0]+ ' ' +sent[i-1][0]+ ' ' +sent[i][0]).lower #this is the token and the token before\n",
    "                #if this word and word before is in the ontology then \n",
    "#                if two_words_before in materials_list: \n",
    "#                    two_words_before_in_materials = True \n",
    "#                else:\n",
    "#                    two_words_before_in_materials = False\n",
    "\n",
    "#                 if two_words_before in taxon_list: \n",
    "#                     two_words_before_in_taxon = True \n",
    "#                 else:\n",
    "#                     two_words_before_in_taxon = False\n",
    "\n",
    "#                if two_words_before in animal_list: \n",
    "#                    two_words_before_in_animal = True \n",
    "#                    print(word)\n",
    "#                else:\n",
    "#                    two_words_before_in_animal = False\n",
    "\n",
    "                if two_words_before in periods_list: \n",
    "                    two_words_before_in_periods = True \n",
    "                else:\n",
    "                    two_words_before_in_periods = False\n",
    "\n",
    "#                if two_words_before in context_list: \n",
    "#                    two_words_before_in_context = True \n",
    "#                else:\n",
    "#                    two_words_before_in_context = False\n",
    "                features.update({\n",
    "                    '-1:word.lower()': word1.lower(), # tells if the token is lower case\n",
    "                    '-1:word.istitle()': word1.istitle(), # tells if the token is capital first letter\n",
    "                    '-1:word.isdigit()': word1.isdigit(), # tells if the toekn is only numbers\n",
    "                    '-1:word.isupper()': word1.isupper(),# tells if the whole token is uppercase\n",
    "#                    'two_words_before_in_materials': two_words_before_in_materials,\n",
    "#                     'two_words_before_in_taxon': two_words_before_in_taxon,\n",
    "#                     'two_words_before_in_animal': two_words_before_in_animal,\n",
    "#                    'two_words_before_in_periods': two_words_before_in_periods,\n",
    "#                    'two_words_before_in_context': two_words_before_in_context,\n",
    "                    '-2-postag': postag3, # what was its POS tag\n",
    "                    '-2:postag[:2]': postag3[:2], #what is the first three POS tag of the word before\n",
    "                })\n",
    "        else:\n",
    "            features['BOS2'] = True # if word is the beggining of sentence label it as so         \n",
    "    else:\n",
    "        features['BOS'] = True # if word is the beggining of sentence label it as so \n",
    "        \n",
    "    if i < len(sent)-1: # is the word at the end of the sentence. sme as above after\n",
    "        wordafter= (sent[i][0]+ ' ' +sent[i+1][0]).lower\n",
    "        #this tells if the token AFTER and each token combined is in the ontology or not\n",
    "#        if wordafter in materials_list: \n",
    "#            wordafter_in_materials = True \n",
    "#        else:\n",
    "#            wordafter_in_materials = False\n",
    "\n",
    "#         if wordafter in taxon_list: \n",
    "#             wordafter_in_taxon = True \n",
    "#         else:\n",
    "#             wordafter_in_taxon = False\n",
    "\n",
    "#        if wordafter in animal_list: \n",
    "#            wordafter_in_animal = True \n",
    "#        else:\n",
    "#            wordafter_in_animal = False\n",
    "\n",
    "        if wordafter in periods_list: \n",
    "            wordafter_in_periods = True \n",
    "        else:\n",
    "            wordafter_in_periods = False\n",
    "\n",
    "#        if wordafter in context_list: \n",
    "#            wordafter_in_context = True \n",
    "#        else:\n",
    "#            wordafter_in_context = False\n",
    "        word2 = sent[i+1][0]  \n",
    "        postag2 = sent[i+1][1] \n",
    "        features.update({\n",
    "#            'wordafter_in_materials': wordafter_in_materials,\n",
    "#             'wordafter_in_taxon': wordafter_in_taxon,\n",
    "#             'wordafter_in_animal': wordafter_in_animal,\n",
    "#            'wordafter_in_periods': wordafter_in_periods,\n",
    "#            'wordafter_in_context': wordafter_in_context,\n",
    "            '+2:word.lower()': word2.lower(),\n",
    "            '+2:word.istitle()': word2.istitle(),\n",
    "            '+2:word.isupper()': word2.isupper(),\n",
    "            '+2:postag': postag2,\n",
    "            '+2:postag[:2]': postag2[:2],\n",
    "        })\n",
    "        \n",
    "        if i < len(sent)-2: #if the token is not at the start of a sentence\n",
    "                word4 = sent[i-1][0] # works out details of the token before - this is to understand the context \n",
    "                postag4 = sent[i-1][1] #what is the postag of the word before\n",
    "                two_words_after = (sent[i][0]+ ' ' +sent[i+1][0]+ ' ' +sent[i+1][0]).lower#this is the token and the token before\n",
    "                #if this word and word before is in the ontology then \n",
    "#                if two_words_after in materials_list: \n",
    "#                    two_words_after_in_materials = True \n",
    "#                else:\n",
    "#                    two_words_after_in_materials = False\n",
    "\n",
    "#                 if two_words_after in taxon_list: \n",
    "#                     two_words_after_in_taxon = True \n",
    "#                 else:\n",
    "#                     two_words_after_in_taxon = False\n",
    "\n",
    "#                if two_words_after in animal_list: \n",
    "#                    two_words_after_in_animal = True \n",
    "#                    print(word)\n",
    "#                else:\n",
    "#                    two_words_after_in_animal = False\n",
    "\n",
    "                if two_words_after in periods_list: \n",
    "                    two_words_after_in_periods = True \n",
    "                else:\n",
    "                   two_words_after_in_periods = False\n",
    "\n",
    "#                if two_words_after in context_list: \n",
    "#                    two_words_after_in_context = True \n",
    "#                else:\n",
    "#                    two_words_after_in_context = False\n",
    "                features.update({\n",
    "                    '-1:word.lower()': word4.lower(), # tells if the token is lower case\n",
    "                    '-1:word.istitle()': word4.istitle(), # tells if the token is capital first letter\n",
    "                    '-1:word.isdigit()': word4.isdigit(), # tells if the toekn is only numbers\n",
    "                    '-1:word.isupper()': word4.isupper(),# tells if the whole token is uppercase\n",
    "#                    'two_words_after_in_materials': two_words_after_in_materials,\n",
    "#                     'two_words_after_in_taxon': two_words_after_in_taxon,\n",
    "#                     'two_words_after_in_animal': two_words_after_in_animal,\n",
    "#                    'two_words_after_in_periods': two_words_after_in_periods,\n",
    "#                    'two_words_after_in_context': two_words_after_in_context,\n",
    "                    '-2-postag': postag4, # what was its POS tag\n",
    "                    '-2:postag[:2]': postag4[:2], #what is the first three POS tag of the word before\n",
    "                })\n",
    "        else:\n",
    "            features['EOS2'] = True # if word is the beggining of sentence label it as so       \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        features['EOS'] = True # if word is the end of sentence label it as so         \n",
    "   \n",
    "\n",
    "    if i < len(sent)-1 and i > 0: # is the word at the end of the sentence. sme as above after\n",
    "            word3 = sent[i+1][0]  \n",
    "            postag3 = sent[i+1][1] \n",
    "            wordsorround = (sent[i-1][0]+ ' ' +sent[i][0]+ ' ' +sent[i+1][0]).lower\n",
    "#            if wordsorround in materials_list: \n",
    "#                wordsorround_in_materials = True \n",
    "#            else:\n",
    "#                wordsorround_in_materials = False\n",
    "\n",
    "#             if wordsorround in taxon_list: \n",
    "#                 wordsorround_in_taxon = True \n",
    "#             else:\n",
    "#                 wordsorround_in_taxon = False\n",
    "\n",
    "#            if wordsorround in animal_list: \n",
    "#                wordsorround_in_animal = True \n",
    "#            else:\n",
    "#                wordsorround_in_animal = False\n",
    "\n",
    "            if wordsorround in periods_list: \n",
    "                wordsorround_in_periods = True \n",
    "            else:\n",
    "                wordsorround_in_periods = False\n",
    "\n",
    "#            if wordsorround in context_list: \n",
    "#                wordsorround_in_context = True \n",
    "#            else:\n",
    "#                wordsorround_in_context = False\n",
    "            features.update({\n",
    "#                'wordsorround_in_materials': wordsorround_in_materials,\n",
    "#                 'wordsorround_in_taxon': wordsorround_in_taxon,\n",
    "#                 'wordsorround_in_animal': wordsorround_in_animal,\n",
    "#                'wordsorround_in_periods': wordsorround_in_periods,\n",
    "#                'wordsorround_in_context': wordsorround_in_context,\n",
    "            })\n",
    "            if i < len(sent)-2 and i > 1: # is the word at the end of the sentence. sme as above after\n",
    "                word5 = sent[i+1][0]  \n",
    "                postag5 = sent[i+1][1] \n",
    "                twowordsorround = (sent[i-2][0]+ ' ' +sent[i-1][0]+ ' ' +sent[i][0]+ ' ' +sent[i+1][0]+ ' ' +sent[i+2][0]).lower\n",
    "#                if twowordsorround in materials_list: \n",
    "#                    twowordsorround_in_materials = True \n",
    "#                else:\n",
    "#                    twowordsorround_in_materials = False\n",
    "\n",
    "#                 if twowordsorround in taxon_list: \n",
    "#                     twowordsorround_in_taxon = True \n",
    "#                 else:\n",
    "#                     twowordsorround_in_taxon = False\n",
    "\n",
    "#                if twowordsorround in animal_list: \n",
    "#                    twowordsorround_in_animal = True \n",
    "#                else:\n",
    "#                    twowordsorround_in_animal = False\n",
    "\n",
    "                if twowordsorround in periods_list: \n",
    "                    twowordsorround_in_periods = True \n",
    "                else:\n",
    "                    twowordsorround_in_periods = False\n",
    "\n",
    "#                if twowordsorround in context_list: \n",
    "#                    twowordsorround_in_context = True \n",
    "#                else:\n",
    "#                    twowordsorround_in_context = False\n",
    "                features.update({\n",
    "#                    'twowordsorround_in_materials': twowordsorround_in_materials,\n",
    "#                     'twowordsorround_in_taxon': twowordsorround_in_taxon,\n",
    "#                     'twowordsorround_in_animal': twowordsorround_in_animal,\n",
    "#                    'twowordsorround_in_periods': twowordsorround_in_periods,\n",
    "#                    'twowordsorround_in_context': twowordsorround_in_context,                   \n",
    "                })\n",
    "            else:\n",
    "                    features['OWS'] = True # if word is the end of sentence label it as so \n",
    "    else:\n",
    "            features['OWS'] = True # if word is the end of sentence label it as so \n",
    "            \n",
    "    return features # output these details\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))] #output for each word\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent] #output for each token\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent] #output for ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sent2features[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'bias': 1.0,\n",
       "   'word.lower()': 'linearbandkeramik',\n",
       "   'word[-3:]': 'mik',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': True,\n",
       "   'postag': 'NOUN',\n",
       "   'postag[:2]': 'NO',\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   '+2:word.lower()': 'aus',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:postag': 'ADP',\n",
       "   '+2:postag[:2]': 'AD',\n",
       "   '-1:word.lower()': '.',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isdigit()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-2-postag': 'PUNCT',\n",
       "   '-2:postag[:2]': 'PU',\n",
       "   'OWS': True},\n",
       "  {'bias': 1.0,\n",
       "   'word.lower()': 'aus',\n",
       "   'word[-3:]': 'aus',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'postag': 'ADP',\n",
       "   'postag[:2]': 'AD',\n",
       "   'word.isdigit()': False,\n",
       "   '-1:word.lower()': 'linearbandkeramik',\n",
       "   '-1:word.istitle()': True,\n",
       "   '-1:word.isdigit()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:postag': 'NOUN',\n",
       "   '-1:postag[:2]': 'NO',\n",
       "   'BOS2': True,\n",
       "   '+2:word.lower()': 'meindling',\n",
       "   '+2:word.istitle()': True,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:postag': 'PROPN',\n",
       "   '+2:postag[:2]': 'PR',\n",
       "   '-2-postag': 'NOUN',\n",
       "   '-2:postag[:2]': 'NO',\n",
       "   'OWS': True},\n",
       "  {'bias': 1.0,\n",
       "   'word.lower()': 'meindling',\n",
       "   'word[-3:]': 'ing',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': True,\n",
       "   'postag': 'PROPN',\n",
       "   'postag[:2]': 'PR',\n",
       "   'word.isdigit()': False,\n",
       "   '-1:word.lower()': 'aus',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isdigit()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:postag': 'ADP',\n",
       "   '-1:postag[:2]': 'AD',\n",
       "   '-2-postag': 'ADP',\n",
       "   '-2:postag[:2]': 'AD',\n",
       "   '+2:word.lower()': ',',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:postag': 'PUNCT',\n",
       "   '+2:postag[:2]': 'PU'},\n",
       "  {'bias': 1.0,\n",
       "   'word.lower()': ',',\n",
       "   'word[-3:]': ',',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'postag': 'PUNCT',\n",
       "   'postag[:2]': 'PU',\n",
       "   'word.isdigit()': False,\n",
       "   '-1:word.lower()': 'meindling',\n",
       "   '-1:word.istitle()': True,\n",
       "   '-1:word.isdigit()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:postag': 'PROPN',\n",
       "   '-1:postag[:2]': 'PR',\n",
       "   '-2-postag': 'PROPN',\n",
       "   '-2:postag[:2]': 'PR',\n",
       "   '+2:word.lower()': 'gem',\n",
       "   '+2:word.istitle()': True,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:postag': 'PROPN',\n",
       "   '+2:postag[:2]': 'PR'},\n",
       "  {'bias': 1.0,\n",
       "   'word.lower()': 'gem',\n",
       "   'word[-3:]': 'Gem',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': True,\n",
       "   'postag': 'PROPN',\n",
       "   'postag[:2]': 'PR',\n",
       "   'word.isdigit()': False,\n",
       "   '-1:word.lower()': ',',\n",
       "   '-1:word.istitle()': False,\n",
       "   '-1:word.isdigit()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:postag': 'PUNCT',\n",
       "   '-1:postag[:2]': 'PU',\n",
       "   '-2-postag': 'PUNCT',\n",
       "   '-2:postag[:2]': 'PU',\n",
       "   '+2:word.lower()': '.',\n",
       "   '+2:word.istitle()': False,\n",
       "   '+2:word.isupper()': False,\n",
       "   '+2:postag': 'PUNCT',\n",
       "   '+2:postag[:2]': 'PU',\n",
       "   'EOS2': True,\n",
       "   'OWS': True},\n",
       "  {'bias': 1.0,\n",
       "   'word.lower()': '.',\n",
       "   'word[-3:]': '.',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'postag': 'PUNCT',\n",
       "   'postag[:2]': 'PU',\n",
       "   'word.isdigit()': False,\n",
       "   '-1:word.lower()': 'gem',\n",
       "   '-1:word.istitle()': True,\n",
       "   '-1:word.isdigit()': False,\n",
       "   '-1:word.isupper()': False,\n",
       "   '-1:postag': 'PROPN',\n",
       "   '-1:postag[:2]': 'PR',\n",
       "   '-2-postag': 'PROPN',\n",
       "   '-2:postag[:2]': 'PR',\n",
       "   'EOS': True,\n",
       "   'OWS': True}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sent2features(train_sent[0:1])[0]\n",
    "[sent2features(s) for s in train_sent[0:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to train the NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sent] # for the token train the ner on the learned set\n",
    "y_train = [sent2labels(s) for s in train_sent] # for the POS tag train the ner on the learned set\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sent] # for the token train the ner on the test set\n",
    "y_test = [sent2labels(s) for s in test_sent] # for the POS tag train the ner on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 35 s\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1,  \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "#crf.fit(X_train, y_train)\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ART',\n",
       " 'B-LOC',\n",
       " 'B-CON',\n",
       " 'B-PER',\n",
       " 'I-ART',\n",
       " 'B-MAT',\n",
       " 'I-PER',\n",
       " 'I-MAT',\n",
       " 'I-CON',\n",
       " 'I-LOC',\n",
       " 'B-SPE',\n",
       " 'I-SPE']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_) # get the list of all labels\n",
    "labels.remove('O') # remove the ones where bio is o - not got a postag\n",
    "labels # show what the labels are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to evaluate its success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5159440641573898"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the f1 score\n",
    "y_pred = crf.predict(X_test) # work out and predict what is the likely token\n",
    "metrics.flat_f1_score(y_test, y_pred, # work out what the likely postag will be, and give it a f1 score\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART       0.65      0.22      0.33       965\n",
      "       I-ART       0.29      0.11      0.16       175\n",
      "       B-CON       0.80      0.53      0.64       993\n",
      "       I-CON       0.00      0.00      0.00       170\n",
      "       B-LOC       0.65      0.30      0.41      1030\n",
      "       I-LOC       0.45      0.27      0.34       279\n",
      "       B-MAT       0.57      0.19      0.29       156\n",
      "       I-MAT       0.00      0.00      0.00         5\n",
      "       B-PER       0.91      0.64      0.75      1564\n",
      "       I-PER       0.70      0.50      0.59       535\n",
      "       B-SPE       0.53      0.41      0.46        22\n",
      "       I-SPE       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.75      0.42      0.54      5895\n",
      "   macro avg       0.46      0.27      0.33      5895\n",
      "weighted avg       0.71      0.42      0.52      5895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate the metrics table\n",
    "# group B and I results - this isnt needed, but orders the list\n",
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "#print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "I-ART  -> I-ART   5.709914\n",
      "B-ART  -> I-ART   5.268014\n",
      "I-MAT  -> I-MAT   5.051077\n",
      "I-PER  -> I-PER   4.775873\n",
      "I-CON  -> I-CON   4.454418\n",
      "B-SPE  -> I-SPE   4.401258\n",
      "B-PER  -> I-PER   4.330284\n",
      "I-SPE  -> I-SPE   4.158327\n",
      "B-MAT  -> I-MAT   4.059485\n",
      "B-CON  -> I-CON   3.797470\n",
      "I-LOC  -> I-LOC   3.660770\n",
      "B-LOC  -> I-LOC   3.124166\n",
      "O      -> O       2.539505\n",
      "O      -> B-PER   1.316442\n",
      "O      -> B-ART   0.911409\n",
      "O      -> B-LOC   0.811756\n",
      "O      -> B-CON   0.515187\n",
      "I-CON  -> O       0.491752\n",
      "B-PER  -> B-CON   0.400941\n",
      "B-MAT  -> B-ART   0.327588\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-LOC  -> I-PER   -2.368283\n",
      "B-ART  -> I-LOC   -2.377898\n",
      "B-ART  -> I-CON   -2.387682\n",
      "I-PER  -> I-CON   -2.453047\n",
      "B-PER  -> I-LOC   -2.469888\n",
      "B-MAT  -> I-CON   -2.560051\n",
      "B-ART  -> I-PER   -2.632507\n",
      "B-LOC  -> I-ART   -2.657297\n",
      "B-CON  -> I-PER   -2.664173\n",
      "B-PER  -> I-ART   -2.838070\n",
      "B-CON  -> I-LOC   -3.122502\n",
      "B-SPE  -> B-PER   -3.161820\n",
      "B-LOC  -> I-CON   -3.681451\n",
      "B-PER  -> I-CON   -4.001483\n",
      "O      -> I-SPE   -4.002238\n",
      "O      -> I-MAT   -4.504281\n",
      "O      -> I-ART   -5.658613\n",
      "O      -> I-PER   -6.411332\n",
      "O      -> I-LOC   -6.706831\n",
      "O      -> I-CON   -6.776857\n"
     ]
    }
   ],
   "source": [
    "#work out which of the transitions are most likely in descending order\n",
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "#what are the 20 most likely transitions\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "#what are the 20 least likely transitions\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "6.524713 B-SPE    word.lower():roggen\n",
      "6.350818 B-SPE    word.lower():menschlichen\n",
      "6.344847 B-PER    word.lower():endneolithische\n",
      "6.322654 B-SPE    word.lower():rinder\n",
      "6.231904 B-SPE    word.lower():menschen\n",
      "6.032314 B-LOC    word.lower():hoch-elten\n",
      "6.015288 B-PER    word[-3:]:kum\n",
      "5.925481 B-LOC    word.lower():dortmund-asseln\n",
      "5.914191 B-LOC    word.lower():schwanfelder\n",
      "5.850276 B-PER    word.lower():bandkeramisch\n",
      "5.753214 B-SPE    word.lower():ziegen\n",
      "5.659569 B-SPE    word.lower():rindern\n",
      "5.612294 B-CON    word.lower():grabhügeln\n",
      "5.607068 B-CON    word.lower():scherbenpackung\n",
      "5.580278 B-PER    word.lower():jahrhundert\n",
      "5.578876 B-ART    word.lower():fässer\n",
      "5.538796 B-MAT    word.lower():eisen\n",
      "5.527776 B-SPE    word.lower():erbsen\n",
      "5.476352 B-LOC    word.lower():straubing-bogen\n",
      "5.383564 B-ART    word.lower():spaltbohlen\n",
      "5.337073 B-ART    word.lower():schwellbalken\n",
      "5.312048 B-CON    word.lower():brandbestattungen\n",
      "5.292746 B-MAT    word.lower():hölzernen\n",
      "5.284173 B-CON    word[-3:]:rab\n",
      "5.201333 B-ART    word.lower():gerate\n",
      "5.155716 B-PER    word.lower():spätneolithische\n",
      "5.104715 B-SPE    word.lower():einkorn\n",
      "5.088009 B-CON    word.lower():moorfund\n",
      "5.060377 B-SPE    word.lower():ölsaaten\n",
      "5.058911 B-PER    word[-3:]:998\n",
      "\n",
      "Top negative:\n",
      "-2.299380 O        -1:word.lower():hofplatz\n",
      "-2.300046 O        word[-3:]:fäß\n",
      "-2.342593 O        word.lower():neolithische\n",
      "-2.343812 O        -1:word.lower():keramik‐\n",
      "-2.363026 O        +2:word.lower():nahtrinne\n",
      "-2.374256 O        -1:word.lower():eggers\n",
      "-2.469239 O        word[-3:]:las\n",
      "-2.470513 O        word[-3:]:wig\n",
      "-2.479536 O        +2:word.lower():rückenlage\n",
      "-2.480153 O        word.lower():kellers\n",
      "-2.515885 O        word.lower():knochenerhaltung\n",
      "-2.521215 O        -1:word.lower():jn\n",
      "-2.528760 O        word.lower():ühlengrund\n",
      "-2.537369 O        word[-3:]:463\n",
      "-2.562325 O        +2:word.lower():chr.\n",
      "-2.606020 O        word.lower():jungneolithischen\n",
      "-2.649183 O        word.lower():halle\n",
      "-2.662336 O        word.lower():holzkeller\n",
      "-2.672875 O        word[-3:]:rge\n",
      "-2.685876 I-ART    -1:word.lower():randständigem\n",
      "-2.734359 O        word.lower():siedlung\n",
      "-2.753364 O        word.lower():eingrabung\n",
      "-2.776980 O        word[-3:]:rne\n",
      "-2.790528 O        word.lower():2015\n",
      "-2.863353 O        word[-3:]:LBK\n",
      "-2.995808 O        -1:word.lower():gelegene\n",
      "-3.009405 O        word.lower():steinsetzung\n",
      "-3.015686 O        word.lower():bestattung\n",
      "-3.179508 O        word.lower():pfosten\n",
      "-3.298083 B-PER    -1:word.lower():zwischen\n"
     ]
    }
   ],
   "source": [
    "#What aspects of the terms make it likely to be that tag\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))    \n",
    "\n",
    "#what are the 30 most likely aspects\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "#what 30 aspects make it least likely to be that term\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional: do hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# this is to work out the best parameters for the testing. not needed yet but can increase the results \n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space, \n",
    "                        cv=3, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        n_iter=50, \n",
    "                        scoring=f1_scorer,\n",
    "                        return_train_score = True)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_  # shows that the best params are \n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d312cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rs.cv_results_[\"mean_test_score\"]\n",
    "rs.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "#_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "#_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "_x = [s['c1'] for s in rs.cv_results_['params']]\n",
    "_y = [s['c2'] for s in rs.cv_results_['params']]\n",
    "_c = [s for s in rs.cv_results_['mean_train_score']]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)\n",
    "))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfe48403845de681c6c88f336067bb2b4650628c5337b500118f83c681650c1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
