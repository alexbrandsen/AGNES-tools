{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c544d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef2ca4a-25ea-46f5-bda3-7c6f97c9fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn_crfsuite in c:\\anaconda\\envs\\trial2\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: six in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from sklearn_crfsuite) (4.64.1)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from sklearn_crfsuite) (0.9.8)\n",
      "Requirement already satisfied: tabulate in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from sklearn_crfsuite) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from tqdm>=2.0->sklearn_crfsuite) (0.4.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\envs\\trial2\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from scikit-learn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: nltk in c:\\anaconda\\envs\\trial2\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from click->nltk) (4.11.3)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from importlib-metadata->click->nltk) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\anaconda\\envs\\trial2\\lib\\site-packages (from importlib-metadata->click->nltk) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "#install required packages\n",
    "!pip install sklearn_crfsuite\n",
    "!pip install scikit-learn \n",
    "!pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cedfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn_crfsuite\n",
    "import os\n",
    "\n",
    "#from matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from itertools\n",
    "from itertools import chain\n",
    "\n",
    "#from sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3450fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a7162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elm', 'felt', 'alabaster', 'spruce', 'tamarac', 'aluminum', 'variscite', 'argillite', 'totternhoe clunch', 'ash', 'aluminium', 'carnelian', 'cornelian', 'plaster', 'sapphire', 'paper', 'ebony', 'garnet', 'rubber', 'coal', 'emerald', 'hazel', 'puddingstone', 'hertfordshire puddingstone', 'charcoal', 'chalk', 'hydrocarbon', 'bakelite', 'amethyst', 'amphibolite', 'larch', 'siltstone', 'mudstone', 'utahlite', 'teak', 'shale', 'ivory', 'marble', 'limestone', 'leather', 'lead', 'lava', 'faience', 'jadeite', 'tooth', 'iron', 'pottery', 'greenstone', 'gold', 'glass', 'flint', 'jet', 'silver', 'pewter', 'obsidian', 'sandstone', 'object material', 'oak', 'mineral', 'wood', 'shell', 'quartz', 'slate', 'steel', 'stone', 'terracotta', 'tin', 'granite', 'quartzite', 'fir', 'antimony', 'schist', 'birch', 'lead alloy', 'zinc', 'dolerite', 'ceramic', 'pine', 'fibreglass', 'glass fibre', 'graphite', 'jade', 'onyx', 'fiberglass', 'beech', 'textile', 'metal', 'alloy', 'bronze', 'horn', 'brass', 'bone', 'antler', 'animal', 'cement', 'chert', 'clay', 'concrete', 'copper', 'amber', 'copper alloy', 'enamel', 'earthenware']\n",
      "\n",
      "\n",
      "\n",
      "['ヒメシンサンカクガイ', 'australmuschel australian', 'brooch clam', 'große flussperlmuschel', 'riesen-flussperlmuschel', \"spengler's freshwater mussel\", 'giant european freshwater pearl mussel', 'louisiana pearlshell', 'western pearlshell', 'westliche flussperlmuschel', 'western freshwater pearl mussel', 'alabama pearlshell', 'eastern pearlshell', 'flussperlmuschel', 'scottish pearl mussel', 'freshwater pearl mussel', 'ウスモシオ', 'スダレモシオ', 'モシオガイ', 'ワタゾコモシオ', 'サガミモシオガイ', 'zwinkokkel', 'フミガイ', 'ハタウネフミガイ', 'rudder cardita', 'トマヤガイ', 'ニクイロトマヤガイ', 'rectangular false cockle', 'クロフトマヤガイ', 'large-ribbed cardita', 'クロマルフミガイ', 'vénéricarde boréale', 'new england cyclocardia', 'north pacific bobtail squid', 'north pacific bobtail squid', 'bouzuika', 'globito del pacífico boreal', 'sépiole du pacifique boréal', 'globito de tasmania', 'sarura', 'southern bobtail squid', 'sépiole du tasmanie', 'mimika bobtail', 'mimika bobtail squid', 'mò-shì-sì-pán-êr-wu-zéi', 'globito mimika', 'sépiole mimika', 'ミミイカ', 'bèi-ruì-shì-sì-pán-êr-wu-zéi', 'double-ear bobtail', 'globito colibri', 'humming-bird bobtail squid', 'leung yee jai', 'niyori-mimi-ika', 'sépiole colibri', 'ニヨリミミイカ', 'brenner’s bobtail', 'burenā-mimika', 'rì-bĕn-àn-êr-wu-zéi', 'チョウチンイカ', 'kleine sepiette', 'linsen-sepiette', 'cappuccetto', 'globito pequeño', 'lentil bobtail', 'lentil bobtail squid', 'seppiola minore', 'sépiole bobie', 'σουπίτσα', 'σουπίτσα φακή', 'elegante sepiette', 'elegant bobtail', 'elegant bobtail squid', 'sepieta elegante', 'sépiole élégante', 'συμμετρική σουπίτσα', 'obskure sepiette', 'sepiette', 'mysterious bobtail squid', 'ramshorn', 'sepieta misteriosa', 'seppiola misteriosa', 'sépiole mystérieuse', 'σουπίτσα αίνιγμα', 'große sepiette', 'cappuccetto', 'common bobtail', 'common bobtail squid', 'greater cuttlefish', 'langwerpige dwerginktvis', 'sepieta común', 'seppiola comune', 'supion', 'sépiole', 'sépiole commune', 'σουπίτσα', 'atlantic bobtail', 'atlantic bobtail squid', 'atlantic cuttlefish', 'atlantik-stummelschwanzsepi']\n",
      "\n",
      "\n",
      "\n",
      "['la tène c', 'la tène d', 'la tène d1', 'la tène d2', 'silla kingdom', 'koguryo kingdom', 'paekche kingdom', 'sasanian', 'unified silla dynasty', 'kushano-sasanian', 'western turks', 'alchon', 'indo-greek', 'hun', 'greco-bactrian', 'epi-palaeolithic', 'hephthalite', 'tularosa', 'afsharid dynasty', 'old assyrian', 'late babylonian', 'neo-babylonian dynasty', 'dilmun', 'punuk', 'old bering sea culture', 'inca', 'late bronze age/early iron age', 'romano-british', 'early medieval', 'pagan', 'sonso', 'gaudo', 'early archaic', 'middle archaic', 'punic', 'kushan', 'bmac', 'rimac', 'calima', 'san agustin', 'tumaco', 'tumaco-la tolita', 'early quimbaya', 'quimbaya', 'yotoco', 'zenu', 'early tolima', 'san agustin regional classic', 'tolima', 'tairona', 'muisca', 'narino', 'late quimbaya', 'serrania de san jacinto', 'tairona period', 'cauca', 'proto-corinthian', 'early minoan', 'early minoan i', 'early minoan ii', 'early minoan iia', 'early minoan iib', 'early minoan iii', 'middle minoan', 'middle minoan i', 'middle minoan ia', 'middle minoan ib', 'middle minoan ii', 'middle minoan iia', 'middle minoan iib', 'middle minoan iii', 'middle minoan iiia', 'middle minoan iiib', 'late minoan', 'late minoan i', 'late minoan ia', 'late minoan ib', 'late minoan ii', 'late minoan iii', 'late minoan iiia', 'late minoan iiia1', 'late minoan iiia2', 'late minoan iiib', 'late minoan iiib1', 'late minoan iiib2', 'late minoan iiic', 'sub-minoan', 'early cycladic', 'grotta-pelos culture', 'keros-syros culture', 'phylakopi i culture', 'middle cycladic', 'late cycladic', 'early cypriot', 'early cypriot i', 'early cypriot ii', 'early cypriot iii', 'middle cypriot', 'late cypriot', 'late cypriot ib']\n",
      "\n",
      "\n",
      "\n",
      "['post-hole', 'post-hole', 'post-hole', 'post-hole', 'post-hole', 'building', 'building', 'building', 'building', 'building', 'building', 'corn-drying oven', 'corn-drying oven', 'corn-drying oven', 'corn-drying oven', 'corn-drying oven', 'corn-drying oven', 'ditch overall', 'ditch overall', 'ditch overall', 'ditch overall', 'ditch overall', 'ditch overall', 'drain', 'drain', 'drain', 'drain', 'drain', 'drain', 'four-post structure', 'four-post structure', 'four-post structure', 'four-post structure', 'four-post structure', 'four-post structure', 'hearth', 'hearth', 'hearth', 'hearth', 'hearth', 'hearth', 'kiln', 'kiln', 'kiln', 'kiln', 'kiln', 'kiln', 'oven', 'oven', 'oven', 'oven', 'oven', 'oven', 'road', 'road', 'road', 'road', 'road', 'road', 'structure', 'structure', 'structure', 'structure', 'structure', 'structure', 'well', 'well', 'well', 'well', 'well', 'well', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'animal disturbance', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'ard mark', 'bank', 'bank', 'bank', 'bank', 'bank']\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\ipykernel_launcher.py:35: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"
     ]
    }
   ],
   "source": [
    "#time to load the ontologies \n",
    "\n",
    "#Material ontology\n",
    "materials = pd.read_csv(r'ontologies\\Materials.csv') #open the file\n",
    "materials[materials.columns[2]]= materials[materials.columns[2]].str.lower() #takes the second column (the one with the entities) and makes it all lower case\n",
    "materials_list = materials[materials.columns[2]].values.tolist() #turns the items into a list\n",
    "print(materials_list) # outputs the list of terms to see what sort of data it contains\n",
    "print(\"\\n\"\"\\n\")\n",
    "\n",
    "# #Taxon ontology\n",
    "# taxon = pd.read_csv(r'ontologies\\Taxon.tsv', sep=\"\\t\", error_bad_lines=False) #open the file\n",
    "# taxon[taxon.columns[8]]= taxon[taxon.columns[8]].str.lower() #takes the second column (the one with the entities) and makes it all lower case\n",
    "# taxon_list = taxon[taxon.columns[8]].values.tolist() #turns the items into a list\n",
    "# print(taxon_list[0:100]) # outputs the list of terms to see what sort of data it contains\n",
    "# print(\"\\n\"\"\\n\")\n",
    "\n",
    "#Animal ontology\n",
    "animal = pd.read_csv(r'ontologies\\VernacularName.tsv', sep=\"\\t\") #open the file\n",
    "animal[animal.columns[2]]= animal[animal.columns[2]].str.lower() #takes the second column (the one with the entities) and makes it all lower case\n",
    "animal_list = animal[animal.columns[2]].values.tolist() #turns the items into a list\n",
    "print(animal_list[0:100]) # outputs the list of terms to see what sort of data it contains\n",
    "print(\"\\n\"\"\\n\")\n",
    "\n",
    "\n",
    "#Periods ontology - for notes see above\n",
    "periods = pd.read_csv(r'ontologies\\Periods.csv')\n",
    "periods[periods.columns[1]]= periods[periods.columns[1]].str.lower()\n",
    "periods_list = periods[periods.columns[1]].values.tolist()\n",
    "print(periods_list[0:100])\n",
    "print(\"\\n\"\"\\n\")\n",
    "\n",
    "\n",
    "#Context ontology - for notes see material \n",
    "context = pd.read_csv(r'ontologies\\context.csv')\n",
    "context[context.columns[0]]= context[context.columns[0]].str.lower().str.replace('*','').str.replace('<','').str.replace('>','') #replace all other symbols\n",
    "context[context.columns[0]]= context[context.columns[0]].replace(to_replace =':.*',value='',regex=True) # replace the words after a colon\n",
    "context_list = context[context.columns[0]].values.tolist()\n",
    "print(context_list[0:100])\n",
    "print(\"\\n\"\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707032b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function turns the file into a list. \n",
    "def file2list(fileLocation):\n",
    "    outputList = []\n",
    "    with open(fileLocation, 'r', encoding='utf8') as myfile:\n",
    "        sentences = myfile.read().split('\\n\\n')\n",
    "        if len(sentences):\n",
    "            for sentence in sentences:\n",
    "                    sentenceList = []\n",
    "                    words = sentence.split('\\n')\n",
    "                    for word in words:\n",
    "                        wordsList = []\n",
    "                        attributes = word.split(' ')\n",
    "                        for attribute in attributes:\n",
    "                            wordsList.append(attribute)\n",
    "                        sentenceList.append(wordsList)\n",
    "                    outputList.append(sentenceList)\n",
    "    \n",
    "    return outputList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3576042-0d57-41e4-a704-2ef79460f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this calls the function\n",
    "def word2features(sent, i): \n",
    "    word = sent[i][0] #takes each token\n",
    "    postag = sent[i][1] #this is the Part of Speach Tager\n",
    "    \n",
    "     #this tells if each token is in the ontology or not \n",
    "    if word in materials_list: \n",
    "        in_materials = True \n",
    "    else:\n",
    "        in_materials = False\n",
    "    \n",
    "#     if word in taxon_list: \n",
    "#         in_taxon = True \n",
    "#     else:\n",
    "#         in_taxon = False\n",
    "    \n",
    "    if word in animal_list: \n",
    "        in_animal = True \n",
    "    else:\n",
    "        in_animal = False\n",
    "  \n",
    "    if word in periods_list: \n",
    "        in_periods = True \n",
    "    else:\n",
    "        in_periods = False\n",
    "\n",
    "    if word in context_list: \n",
    "        in_context = True \n",
    "    else:\n",
    "        in_context = False\n",
    "    \n",
    "    #time to give each token some information     \n",
    "    features = { # these are all default. \n",
    "        'bias': 1.0, # bias is just 1. \n",
    "        'word.lower()': word.lower(), # tells if the token is lower case \n",
    "        'word[-3:]': word[-3:], # takes the last four letters - the suffix\n",
    "        'Word.in_materials': in_materials, #is the token in the material ontology\n",
    "#         'Word.in_taxon': in_taxon, #is the token in the material ontology\n",
    "        'Word.in_animal': in_animal, #is the token in the periods ontology \n",
    "        'Word.in_periods': in_periods, #is the token in the periods ontology\n",
    "        'Word.in_context': in_context, #is the token in the evidence ontology\n",
    "        'word.isupper()': word.isupper(), # tells if the whole token is uppercase \n",
    "        'word.istitle()': word.istitle(), # tells if the token is capital first letter\n",
    "        'postag': postag,  # what is its label - Part-Of-Speech Tagger\n",
    "        'postag[:2]': postag[:2],  #Takes the first three letters of the tag\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0: #if the token is not at the start of a sentence\n",
    "        word1 = sent[i-1][0] # works out details of the token before - this is to understand the context \n",
    "        postag1 = sent[i-1][1] #what is the postag of the word before\n",
    "        wordbefore = (sent[i-1][0]+ ' ' +sent[i][0]).lower #this is the token and the token before\n",
    "        #if this word and word before is in the ontology then \n",
    "        if wordbefore in materials_list: \n",
    "            wordbefore_in_materials = True \n",
    "        else:\n",
    "            wordbefore_in_materials = False\n",
    "\n",
    "#         if wordbefore in taxon_list: \n",
    "#             wordbefore_in_taxon = True \n",
    "#         else:\n",
    "#             wordbefore_in_taxon = False\n",
    "\n",
    "        if wordbefore in animal_list: \n",
    "            wordbefore_in_animal = True \n",
    "            print(word)\n",
    "        else:\n",
    "            wordbefore_in_animal = False\n",
    "\n",
    "        if wordbefore in periods_list: \n",
    "            wordbefore_in_periods = True \n",
    "        else:\n",
    "            wordbefore_in_periods = False\n",
    "\n",
    "        if wordbefore in context_list: \n",
    "            wordbefore_in_context = True \n",
    "        else:\n",
    "            wordbefore_in_context = False\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(), # tells if the token is lower case\n",
    "            '-1:word.istitle()': word1.istitle(), # tells if the token is capital first letter\n",
    "            '-1:word.isdigit()': word1.isdigit(), # tells if the toekn is only numbers\n",
    "            '-1:word.isupper()': word1.isupper(),# tells if the whole token is uppercase\n",
    "            'wordbefore_in_materials': wordbefore_in_materials,\n",
    "#             'wordbefore_in_taxon': wordbefore_in_taxon,\n",
    "            'wordbefore_in_animal': wordbefore_in_animal,\n",
    "            'wordbefore_in_periods': wordbefore_in_periods,\n",
    "            'wordbefore_in_context': wordbefore_in_context,\n",
    "            '-1:postag': postag1, # what was its POS tag\n",
    "            '-1:postag[:2]': postag1[:2], #what is the first three POS tag of the word before\n",
    "        })\n",
    "    \n",
    "        if i > 1: #if the token is not at the start of a sentence\n",
    "                word3 = sent[i-1][0] # works out details of the token before - this is to understand the context \n",
    "                postag3 = sent[i-1][1] #what is the postag of the word before\n",
    "                two_words_before = (sent[i-1][0]+ ' ' +sent[i-1][0]+ ' ' +sent[i][0]).lower #this is the token and the token before\n",
    "                #if this word and word before is in the ontology then \n",
    "                if two_words_before in materials_list: \n",
    "                    two_words_before_in_materials = True \n",
    "                else:\n",
    "                    two_words_before_in_materials = False\n",
    "\n",
    "#                 if two_words_before in taxon_list: \n",
    "#                     two_words_before_in_taxon = True \n",
    "#                 else:\n",
    "#                     two_words_before_in_taxon = False\n",
    "\n",
    "                if two_words_before in animal_list: \n",
    "                    two_words_before_in_animal = True \n",
    "                    print(word)\n",
    "                else:\n",
    "                    two_words_before_in_animal = False\n",
    "\n",
    "                if two_words_before in periods_list: \n",
    "                    two_words_before_in_periods = True \n",
    "                else:\n",
    "                    two_words_before_in_periods = False\n",
    "\n",
    "                if two_words_before in context_list: \n",
    "                    two_words_before_in_context = True \n",
    "                else:\n",
    "                    two_words_before_in_context = False\n",
    "                features.update({\n",
    "                    '-1:word.lower()': word1.lower(), # tells if the token is lower case\n",
    "                    '-1:word.istitle()': word1.istitle(), # tells if the token is capital first letter\n",
    "                    '-1:word.isdigit()': word1.isdigit(), # tells if the toekn is only numbers\n",
    "                    '-1:word.isupper()': word1.isupper(),# tells if the whole token is uppercase\n",
    "                    'two_words_before_in_materials': two_words_before_in_materials,\n",
    "#                     'two_words_before_in_taxon': two_words_before_in_taxon,\n",
    "                    'two_words_before_in_animal': two_words_before_in_animal,\n",
    "                    'two_words_before_in_periods': two_words_before_in_periods,\n",
    "                    'two_words_before_in_context': two_words_before_in_context,\n",
    "                    '-2-postag': postag3, # what was its POS tag\n",
    "                    '-2:postag[:2]': postag3[:2], #what is the first three POS tag of the word before\n",
    "                })\n",
    "        else:\n",
    "            features['BOS2'] = True # if word is the beggining of sentence label it as so         \n",
    "    else:\n",
    "        features['BOS'] = True # if word is the beggining of sentence label it as so \n",
    "        \n",
    "    if i < len(sent)-1: # is the word at the end of the sentence. sme as above after\n",
    "        wordafter= (sent[i][0]+ ' ' +sent[i+1][0]).lower\n",
    "        #this tells if the token AFTER and each token combined is in the ontology or not\n",
    "        if wordafter in materials_list: \n",
    "            wordafter_in_materials = True \n",
    "        else:\n",
    "            wordafter_in_materials = False\n",
    "\n",
    "#         if wordafter in taxon_list: \n",
    "#             wordafter_in_taxon = True \n",
    "#         else:\n",
    "#             wordafter_in_taxon = False\n",
    "\n",
    "        if wordafter in animal_list: \n",
    "            wordafter_in_animal = True \n",
    "        else:\n",
    "            wordafter_in_animal = False\n",
    "\n",
    "        if wordafter in periods_list: \n",
    "            wordafter_in_periods = True \n",
    "        else:\n",
    "            wordafter_in_periods = False\n",
    "\n",
    "        if wordafter in context_list: \n",
    "            wordafter_in_context = True \n",
    "        else:\n",
    "            wordafter_in_context = False\n",
    "        word2 = sent[i+1][0]  \n",
    "        postag2 = sent[i+1][1] \n",
    "        features.update({\n",
    "            'wordafter_in_materials': wordafter_in_materials,\n",
    "#             'wordafter_in_taxon': wordafter_in_taxon,\n",
    "            'wordafter_in_animal': wordafter_in_animal,\n",
    "            'wordafter_in_periods': wordafter_in_periods,\n",
    "            'wordafter_in_context': wordafter_in_context,\n",
    "            '+2:word.lower()': word2.lower(),\n",
    "            '+2:word.istitle()': word2.istitle(),\n",
    "            '+2:word.isupper()': word2.isupper(),\n",
    "            '+2:postag': postag2,\n",
    "            '+2:postag[:2]': postag2[:2],\n",
    "        })\n",
    "        \n",
    "        if i < len(sent)-2: #if the token is not at the start of a sentence\n",
    "                word4 = sent[i-1][0] # works out details of the token before - this is to understand the context \n",
    "                postag4 = sent[i-1][1] #what is the postag of the word before\n",
    "                two_words_after = (sent[i][0]+ ' ' +sent[i+1][0]+ ' ' +sent[i+1][0]).lower#this is the token and the token before\n",
    "                #if this word and word before is in the ontology then \n",
    "                if two_words_after in materials_list: \n",
    "                    two_words_after_in_materials = True \n",
    "                else:\n",
    "                    two_words_after_in_materials = False\n",
    "\n",
    "#                 if two_words_after in taxon_list: \n",
    "#                     two_words_after_in_taxon = True \n",
    "#                 else:\n",
    "#                     two_words_after_in_taxon = False\n",
    "\n",
    "                if two_words_after in animal_list: \n",
    "                    two_words_after_in_animal = True \n",
    "                    print(word)\n",
    "                else:\n",
    "                    two_words_after_in_animal = False\n",
    "\n",
    "                if two_words_after in periods_list: \n",
    "                    two_words_after_in_periods = True \n",
    "                else:\n",
    "                   two_words_after_in_periods = False\n",
    "\n",
    "                if two_words_after in context_list: \n",
    "                    two_words_after_in_context = True \n",
    "                else:\n",
    "                    two_words_after_in_context = False\n",
    "                features.update({\n",
    "                    '-1:word.lower()': word4.lower(), # tells if the token is lower case\n",
    "                    '-1:word.istitle()': word4.istitle(), # tells if the token is capital first letter\n",
    "                    '-1:word.isdigit()': word4.isdigit(), # tells if the toekn is only numbers\n",
    "                    '-1:word.isupper()': word4.isupper(),# tells if the whole token is uppercase\n",
    "                    'two_words_after_in_materials': two_words_after_in_materials,\n",
    "#                     'two_words_after_in_taxon': two_words_after_in_taxon,\n",
    "                    'two_words_after_in_animal': two_words_after_in_animal,\n",
    "                    'two_words_after_in_periods': two_words_after_in_periods,\n",
    "                    'two_words_after_in_context': two_words_after_in_context,\n",
    "                    '-2-postag': postag4, # what was its POS tag\n",
    "                    '-2:postag[:2]': postag4[:2], #what is the first three POS tag of the word before\n",
    "                })\n",
    "        else:\n",
    "            features['EOS2'] = True # if word is the beggining of sentence label it as so       \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        features['EOS'] = True # if word is the end of sentence label it as so         \n",
    "   \n",
    "\n",
    "    if i < len(sent)-1 and i > 0: # is the word at the end of the sentence. sme as above after\n",
    "            word3 = sent[i+1][0]  \n",
    "            postag3 = sent[i+1][1] \n",
    "            wordsorround = (sent[i-1][0]+ ' ' +sent[i][0]+ ' ' +sent[i+1][0]).lower\n",
    "            if wordsorround in materials_list: \n",
    "                wordsorround_in_materials = True \n",
    "            else:\n",
    "                wordsorround_in_materials = False\n",
    "\n",
    "#             if wordsorround in taxon_list: \n",
    "#                 wordsorround_in_taxon = True \n",
    "#             else:\n",
    "#                 wordsorround_in_taxon = False\n",
    "\n",
    "            if wordsorround in animal_list: \n",
    "                wordsorround_in_animal = True \n",
    "            else:\n",
    "                wordsorround_in_animal = False\n",
    "\n",
    "            if wordsorround in periods_list: \n",
    "                wordsorround_in_periods = True \n",
    "            else:\n",
    "                wordsorround_in_periods = False\n",
    "\n",
    "            if wordsorround in context_list: \n",
    "                wordsorround_in_context = True \n",
    "            else:\n",
    "                wordsorround_in_context = False\n",
    "            features.update({\n",
    "                'wordsorround_in_materials': wordsorround_in_materials,\n",
    "#                 'wordsorround_in_taxon': wordsorround_in_taxon,\n",
    "                'wordsorround_in_animal': wordsorround_in_animal,\n",
    "                'wordsorround_in_periods': wordsorround_in_periods,\n",
    "                'wordsorround_in_context': wordsorround_in_context,\n",
    "            })\n",
    "            if i < len(sent)-2 and i > 1: # is the word at the end of the sentence. sme as above after\n",
    "                word5 = sent[i+1][0]  \n",
    "                postag5 = sent[i+1][1] \n",
    "                twowordsorround = (sent[i-2][0]+ ' ' +sent[i-1][0]+ ' ' +sent[i][0]+ ' ' +sent[i+1][0]+ ' ' +sent[i+2][0]).lower\n",
    "                if twowordsorround in materials_list: \n",
    "                    twowordsorround_in_materials = True \n",
    "                else:\n",
    "                    twowordsorround_in_materials = False\n",
    "\n",
    "#                 if twowordsorround in taxon_list: \n",
    "#                     twowordsorround_in_taxon = True \n",
    "#                 else:\n",
    "#                     twowordsorround_in_taxon = False\n",
    "\n",
    "                if twowordsorround in animal_list: \n",
    "                    twowordsorround_in_animal = True \n",
    "                else:\n",
    "                    twowordsorround_in_animal = False\n",
    "\n",
    "                if twowordsorround in periods_list: \n",
    "                    twowordsorround_in_periods = True \n",
    "                else:\n",
    "                    twowordsorround_in_periods = False\n",
    "\n",
    "                if twowordsorround in context_list: \n",
    "                    twowordsorround_in_context = True \n",
    "                else:\n",
    "                    twowordsorround_in_context = False\n",
    "                features.update({\n",
    "                    'twowordsorround_in_materials': twowordsorround_in_materials,\n",
    "#                     'twowordsorround_in_taxon': twowordsorround_in_taxon,\n",
    "                    'twowordsorround_in_animal': twowordsorround_in_animal,\n",
    "                    'twowordsorround_in_periods': twowordsorround_in_periods,\n",
    "                    'twowordsorround_in_context': twowordsorround_in_context,                   \n",
    "                })\n",
    "            else:\n",
    "                    features['OWS'] = True # if word is the end of sentence label it as so \n",
    "    else:\n",
    "            features['OWS'] = True # if word is the end of sentence label it as so \n",
    "            \n",
    "    return features # output these details\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))] #output for each word\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent] #output for each token\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent] #output for ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be1aa34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.758     0.479     0.587       470\n",
      "       I-ART      0.342     0.562     0.425        96\n",
      "       B-CON      0.825     0.426     0.562       399\n",
      "       I-CON      0.485     0.281     0.356        57\n",
      "       B-LOC      0.865     0.557     0.677       115\n",
      "       I-LOC      0.736     0.574     0.645        68\n",
      "       B-MAT      0.438     0.111     0.177        63\n",
      "       I-MAT      0.500     0.077     0.133        13\n",
      "       B-PER      0.923     0.806     0.860       608\n",
      "       I-PER      0.934     0.762     0.840       669\n",
      "       B-SPE      1.000     0.193     0.323        83\n",
      "       I-SPE      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.824     0.603     0.696      2641\n",
      "   macro avg      0.650     0.402     0.465      2641\n",
      "weighted avg      0.832     0.603     0.684      2641\n",
      "\n",
      "split1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.688     0.181     0.286       293\n",
      "       I-ART      0.207     0.118     0.150        51\n",
      "       B-CON      0.692     0.115     0.197       235\n",
      "       I-CON      0.143     0.026     0.043        39\n",
      "       B-LOC      0.864     0.253     0.392        75\n",
      "       I-LOC      0.650     0.351     0.456        37\n",
      "       B-MAT      0.143     0.018     0.031        57\n",
      "       I-MAT      0.250     0.077     0.118        13\n",
      "       B-PER      0.816     0.399     0.536       356\n",
      "       I-PER      0.847     0.482     0.614       438\n",
      "\n",
      "   micro avg      0.755     0.297     0.427      1594\n",
      "   macro avg      0.530     0.202     0.282      1594\n",
      "weighted avg      0.717     0.297     0.407      1594\n",
      "\n",
      "split2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.721     0.150     0.249       293\n",
      "       I-ART      0.192     0.098     0.130        51\n",
      "       B-CON      0.811     0.128     0.221       235\n",
      "       I-CON      0.100     0.026     0.041        39\n",
      "       B-LOC      0.759     0.293     0.423        75\n",
      "       I-LOC      0.826     0.514     0.633        37\n",
      "       B-MAT      0.231     0.053     0.086        57\n",
      "       I-MAT      0.000     0.000     0.000        13\n",
      "       B-PER      0.789     0.421     0.549       356\n",
      "       I-PER      0.837     0.505     0.630       438\n",
      "       B-SPE      0.000     0.000     0.000        29\n",
      "\n",
      "   micro avg      0.758     0.305     0.435      1623\n",
      "   macro avg      0.479     0.199     0.269      1623\n",
      "weighted avg      0.717     0.305     0.409      1623\n",
      "\n",
      "split3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.694     0.232     0.348       293\n",
      "       I-ART      0.226     0.275     0.248        51\n",
      "       B-CON      0.789     0.128     0.220       235\n",
      "       I-CON      0.125     0.026     0.043        39\n",
      "       B-LOC      0.767     0.307     0.438        75\n",
      "       I-LOC      0.826     0.514     0.633        37\n",
      "       B-MAT      0.250     0.035     0.062        57\n",
      "       I-MAT      0.500     0.077     0.133        13\n",
      "       B-PER      0.787     0.478     0.594       356\n",
      "       I-PER      0.840     0.527     0.648       438\n",
      "       B-SPE      0.000     0.000     0.000        29\n",
      "\n",
      "   micro avg      0.736     0.344     0.469      1623\n",
      "   macro avg      0.528     0.236     0.306      1623\n",
      "weighted avg      0.716     0.344     0.447      1623\n",
      "\n",
      "split4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.639     0.259     0.369       293\n",
      "       I-ART      0.091     0.098     0.094        51\n",
      "       B-CON      0.872     0.174     0.291       235\n",
      "       I-CON      0.000     0.000     0.000        39\n",
      "       B-LOC      0.769     0.400     0.526        75\n",
      "       I-LOC      0.625     0.541     0.580        37\n",
      "       B-MAT      0.333     0.035     0.063        57\n",
      "       I-MAT      0.000     0.000     0.000        13\n",
      "       B-PER      0.829     0.559     0.668       356\n",
      "       I-PER      0.870     0.610     0.717       438\n",
      "       B-SPE      0.000     0.000     0.000        29\n",
      "       I-SPE      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.751     0.394     0.517      1623\n",
      "   macro avg      0.419     0.223     0.276      1623\n",
      "weighted avg      0.723     0.394     0.491      1623\n",
      "\n",
      "split5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.645     0.266     0.377       293\n",
      "       I-ART      0.157     0.157     0.157        51\n",
      "       B-CON      0.776     0.162     0.268       235\n",
      "       I-CON      0.000     0.000     0.000        39\n",
      "       B-LOC      0.810     0.453     0.581        75\n",
      "       I-LOC      0.564     0.595     0.579        37\n",
      "       B-MAT      0.375     0.053     0.092        57\n",
      "       I-MAT      1.000     0.077     0.143        13\n",
      "       B-PER      0.779     0.576     0.662       356\n",
      "       I-PER      0.898     0.562     0.691       438\n",
      "       B-SPE      1.000     0.069     0.129        29\n",
      "       I-SPE      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.746     0.392     0.514      1623\n",
      "   macro avg      0.584     0.247     0.307      1623\n",
      "weighted avg      0.736     0.392     0.490      1623\n",
      "\n",
      "split6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.635     0.273     0.382       293\n",
      "       I-ART      0.114     0.098     0.105        51\n",
      "       B-CON      0.855     0.226     0.357       235\n",
      "       I-CON      0.588     0.256     0.357        39\n",
      "       B-LOC      0.810     0.453     0.581        75\n",
      "       I-LOC      0.515     0.459     0.486        37\n",
      "       B-MAT      0.267     0.070     0.111        57\n",
      "       I-MAT      0.000     0.000     0.000        13\n",
      "       B-PER      0.855     0.680     0.757       356\n",
      "       I-PER      0.916     0.644     0.756       438\n",
      "       B-SPE      1.000     0.069     0.129        29\n",
      "       I-SPE      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.777     0.449     0.569      1623\n",
      "   macro avg      0.546     0.269     0.335      1623\n",
      "weighted avg      0.767     0.449     0.547      1623\n",
      "\n",
      "split7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.691     0.328     0.444       293\n",
      "       I-ART      0.238     0.392     0.296        51\n",
      "       B-CON      0.826     0.243     0.375       235\n",
      "       I-CON      0.474     0.231     0.310        39\n",
      "       B-LOC      0.846     0.440     0.579        75\n",
      "       I-LOC      0.583     0.568     0.575        37\n",
      "       B-MAT      0.333     0.088     0.139        57\n",
      "       I-MAT      0.143     0.077     0.100        13\n",
      "       B-PER      0.881     0.685     0.771       356\n",
      "       I-PER      0.856     0.667     0.750       438\n",
      "       B-SPE      1.000     0.069     0.129        29\n",
      "       I-SPE      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.759     0.481     0.588      1623\n",
      "   macro avg      0.573     0.316     0.372      1623\n",
      "weighted avg      0.771     0.481     0.571      1623\n",
      "\n",
      "split8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.778     0.382     0.513       293\n",
      "       I-ART      0.268     0.431     0.331        51\n",
      "       B-CON      0.744     0.260     0.385       235\n",
      "       I-CON      0.526     0.256     0.345        39\n",
      "       B-LOC      0.878     0.480     0.621        75\n",
      "       I-LOC      0.656     0.568     0.609        37\n",
      "       B-MAT      0.583     0.123     0.203        57\n",
      "       I-MAT      0.500     0.077     0.133        13\n",
      "       B-PER      0.899     0.775     0.833       356\n",
      "       I-PER      0.916     0.651     0.761       438\n",
      "       B-SPE      1.000     0.069     0.129        29\n",
      "       I-SPE      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.806     0.513     0.627      1623\n",
      "   macro avg      0.646     0.339     0.405      1623\n",
      "weighted avg      0.812     0.513     0.608      1623\n",
      "\n",
      "split9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ART      0.826     0.437     0.571       293\n",
      "       I-ART      0.382     0.412     0.396        51\n",
      "       B-CON      0.741     0.268     0.394       235\n",
      "       I-CON      0.500     0.282     0.361        39\n",
      "       B-LOC      0.863     0.587     0.698        75\n",
      "       I-LOC      0.647     0.595     0.620        37\n",
      "       B-MAT      0.538     0.123     0.200        57\n",
      "       I-MAT      0.429     0.231     0.300        13\n",
      "       B-PER      0.893     0.820     0.855       356\n",
      "       I-PER      0.939     0.740     0.828       438\n",
      "       B-SPE      0.750     0.414     0.533        29\n",
      "       I-SPE      0.000     0.000     0.000         0\n",
      "\n",
      "   micro avg      0.835     0.571     0.678      1623\n",
      "   macro avg      0.626     0.409     0.480      1623\n",
      "weighted avg      0.820     0.571     0.658      1623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir('no-tables\\\\txt'):    \n",
    "    print(folder)\n",
    "    #load the datasets, with training documents as train, and test documents and test\n",
    "    train =  file2list('no-tables\\\\txt\\\\'+folder+\"\\\\train.txt\") \n",
    "    test =  file2list('no-tables\\\\txt\\\\'+folder+\"\\\\test.txt\") \n",
    "    #remove empty lines as this breaks the code\n",
    "    test[0].pop() \n",
    "    train[0].pop()\n",
    "#    \n",
    "    # calculates the time to open the file\n",
    "            #train the NER on the list. there is one set of test and one of training. often 20:80 split\n",
    "    train_sent = train\n",
    "    test_sent = test   # tests the sent (input) of the given list as defined above\n",
    "    #train_sent[0] # displayes the first 10 rows in the bio. - each row hs the token (effectively word), followed by pos?, and the bio label\n",
    "    # to identify whats below - token - label(specific label) - common derivitive of word (for posting would be post)  - then the bio label\n",
    "    #sent2features(train_sent[0:1])[0]\n",
    "    [sent2features(s) for s in train_sent[0:1]]\n",
    "    X_train = [sent2features(s) for s in train_sent] # for the token train the ner on the learned set\n",
    "    y_train = [sent2labels(s) for s in train_sent] # for the POS tag train the ner on the learned set\n",
    "\n",
    "    X_test = [sent2features(s) for s in test_sent] # for the token train the ner on the test set\n",
    "    y_test = [sent2labels(s) for s in test_sent] # for the POS tag train the ner on the test set\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs', \n",
    "        c1=0.0020339643465827964,  #was initially 0.1 each\n",
    "        c2=0.028003487848126302, # 'c1': 0.2963053968677204, 'c2': 0.004195898642365605\n",
    "        max_iterations=100, \n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(X_train, y_train)\n",
    "\n",
    "    labels = list(crf.classes_) # get the list of all labels\n",
    "#     print(labels)\n",
    "    labels.remove('O') # remove the ones where bio is o - not got a postag\n",
    "    # labels # show what the labels are \n",
    "    #calculate the f1 score\n",
    "    y_pred = crf.predict(X_test) # work out and predict what is the likely token\n",
    "    metrics.flat_f1_score(y_test, y_pred, # work out what the likely postag will be, and give it a f1 score\n",
    "                          average='weighted', labels=labels)\n",
    "\n",
    "    #calculate the metrics table\n",
    "    # group B and I results - this isnt needed, but orders the list\n",
    "    sorted_labels = sorted(\n",
    "        labels, \n",
    "        key=lambda name: (name[1:], name[0])\n",
    "    )\n",
    "    # print(metrics.classification_report(y_test, y_pred))\n",
    "    print(metrics.flat_classification_report(\n",
    "       y_test, y_pred, labels=sorted_labels, digits=3\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45510c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95bfb9-29bd-4260-907c-6136def7d09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a17316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a22fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to use POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75babd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sent2features[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35321811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29e6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to train the NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1310e2-573f-4064-a10a-4084644572fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0f3875-dddf-4263-9b23-d84d4bccda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e61bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2823dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to evaluate its success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94749648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2525f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "I-ART  -> I-ART   4.926236\n",
      "I-PER  -> I-PER   4.788583\n",
      "B-ART  -> I-ART   4.600642\n",
      "B-CON  -> I-CON   4.204833\n",
      "I-CON  -> I-CON   3.848280\n",
      "B-SPE  -> I-SPE   3.827577\n",
      "B-MAT  -> I-MAT   3.812444\n",
      "I-LOC  -> I-LOC   3.396669\n",
      "O      -> O       3.335598\n",
      "B-PER  -> I-PER   2.781085\n",
      "B-LOC  -> I-LOC   2.384541\n",
      "I-MAT  -> I-MAT   2.154743\n",
      "B-MAT  -> B-ART   1.883056\n",
      "O      -> B-PER   1.763551\n",
      "B-SPE  -> B-ART   1.433932\n",
      "I-ART  -> B-CON   1.351225\n",
      "B-MAT  -> B-CON   1.056310\n",
      "I-MAT  -> B-ART   1.043014\n",
      "I-SPE  -> I-SPE   0.870469\n",
      "I-CON  -> B-CON   0.855547\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-PER  -> B-SPE   -1.840747\n",
      "B-ART  -> I-CON   -1.864372\n",
      "B-LOC  -> B-CON   -1.870458\n",
      "B-CON  -> I-ART   -2.168228\n",
      "I-PER  -> I-ART   -2.185135\n",
      "B-MAT  -> I-CON   -2.201417\n",
      "B-MAT  -> I-ART   -2.239880\n",
      "B-ART  -> B-CON   -2.247380\n",
      "B-ART  -> B-PER   -2.261325\n",
      "O      -> I-SPE   -2.262403\n",
      "B-PER  -> I-CON   -2.506402\n",
      "B-SPE  -> I-ART   -2.640231\n",
      "B-LOC  -> I-PER   -2.748381\n",
      "B-PER  -> I-ART   -2.900018\n",
      "O      -> I-MAT   -3.194909\n",
      "B-PER  -> I-LOC   -3.560409\n",
      "O      -> I-CON   -5.645811\n",
      "O      -> I-ART   -6.018697\n",
      "O      -> I-PER   -6.440062\n",
      "O      -> I-LOC   -6.578055\n"
     ]
    }
   ],
   "source": [
    "#work out which of the transitions are most likely in descending order\n",
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "#what are the 20 most likely transitions\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "#what are the 20 least likely transitions\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df645362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "5.224631 B-PER    word[-3:]:xon\n",
      "5.105987 B-ART    word.lower():coping\n",
      "5.092587 B-CON    word.lower():postholes\n",
      "5.063482 B-ART    word.lower():loomweight\n",
      "4.936461 B-PER    word[-3:]:/05\n",
      "4.771421 B-MAT    word.lower():ragstone\n",
      "4.559046 B-LOC    word.lower():leicester\n",
      "4.446704 O        Word_after:pits or\n",
      "4.429710 B-ART    word.lower():pottery\n",
      "4.398048 O        word_before:of Bedfordshire\n",
      "4.393974 O        word_before:or flint\n",
      "4.391809 B-CON    word.lower():post-holes\n",
      "4.385493 O        Word_after:school had\n",
      "4.369093 B-CON    word_before:( Building\n",
      "4.366871 B-PER    word.lower():post-medieval\n",
      "4.311591 B-CON    Word_after:school to\n",
      "4.285267 B-PER    word[-3:]:val\n",
      "4.283504 B-CON    Word_after:floor was\n",
      "4.110761 B-ART    word_before:charcoal remains\n",
      "4.104962 B-PER    word.lower():romano-british\n",
      "4.094635 B-LOC    word[-3:]:ton\n",
      "4.073056 B-PER    word.lower():roman\n",
      "4.011830 I-PER    word.lower():centuries\n",
      "3.937804 B-ART    word.lower():slates\n",
      "3.925067 B-ART    word.lower():tegulae\n",
      "3.925067 B-ART    word[-3:]:lae\n",
      "3.921473 O        Word_after:flint is\n",
      "3.828591 B-CON    word_before:be fills\n",
      "3.819609 B-CON    word.lower():pits\n",
      "3.809373 B-LOC    word[-3:]:ire\n",
      "\n",
      "Top negative:\n",
      "-2.381684 B-ART    Word_after:pottery is\n",
      "-2.398171 O        word.lower():foundations\n",
      "-2.404778 O        -1:word.lower():peripheral\n",
      "-2.411356 O        Word_after:structure was\n",
      "-2.452167 O        word.lower():pits\n",
      "-2.465694 B-PER    -1:word.lower():late\n",
      "-2.547504 B-PER    Word_after:medieval period\n",
      "-2.562820 O        word[-3:]:val\n",
      "-2.571813 O        word.lower():england\n",
      "-2.603724 O        word[-3:]:eat\n",
      "-2.608545 O        word.lower():gully\n",
      "-2.631036 O        -1:word.lower():fired\n",
      "-2.647527 O        Word_after:features including\n",
      "-2.663991 B-ART    word_before:of pottery\n",
      "-2.675127 O        +2:word.lower():os\n",
      "-2.684252 I-CON    word.isdigit()\n",
      "-2.750543 O        word.lower():pottery\n",
      "-2.772910 O        word[-3:]:ton\n",
      "-2.783753 O        -1:word.lower():rom\n",
      "-2.815835 O        word.lower():wares\n",
      "-2.844857 I-ART    -1:postag:NNS\n",
      "-2.847933 O        word.lower():windows\n",
      "-2.975790 O        word.lower():ragstone\n",
      "-3.006140 O        word.lower():grain\n",
      "-3.045195 O        -1:word.lower():possibly\n",
      "-3.243313 O        word[-3:]:ole\n",
      "-3.340763 O        word.lower():prehistoric\n",
      "-3.362545 O        word.lower():ditches\n",
      "-3.371289 O        word.lower():kent\n",
      "-3.672046 O        word[-3:]:ury\n"
     ]
    }
   ],
   "source": [
    "#What aspects of the terms make it likely to be that tag\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))    \n",
    "\n",
    "#what are the 30 most likely aspects\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "#what 30 aspects make it least likely to be that term\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67a6aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dont run this takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81220ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m         evaluate_candidates(\n\u001b[0;32m   1767\u001b[0m             ParameterSampler(\n\u001b[1;32m-> 1768\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m             )\n\u001b[0;32m   1770\u001b[0m         )\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    849\u001b[0m                     )\n\u001b[0;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[1;32m--> 851\u001b[1;33m                         \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m                     )\n\u001b[0;32m    853\u001b[0m                 )\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\trial2\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    335\u001b[0m                     \u001b[1;34m\"Cannot have number of splits n_splits={0} greater\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                     \u001b[1;34m\" than the number of samples: n_samples={1}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m                 ).format(self.n_splits, n_samples)\n\u001b[0m\u001b[0;32m    338\u001b[0m             )\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=1."
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# this is to work out the best parameters for the testing. not needed yet but can increase the results by .1 \n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space, \n",
    "                        cv=3, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        n_iter=50, \n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af52ece",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17852\\1546018787.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# crf = rs.best_estimator_  # shows that the best params are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best params:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best CV score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model size: {:0.2f}M'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "# crf = rs.best_estimator_  # shows that the best params are \n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)\n",
    "))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940a4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8f133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bbf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf10de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
